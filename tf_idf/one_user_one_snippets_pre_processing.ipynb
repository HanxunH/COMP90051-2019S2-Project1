{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractURL(sentence):\n",
    "    subStr = sentence\n",
    "    urlList = []\n",
    "\n",
    "    if \"@handle\" in subStr:\n",
    "        subStr = subStr.replace(\"@handle\", \"\")\n",
    "    \n",
    "    http_targets = [\"http://\", \"https://\"]\n",
    "    for target in http_targets:\n",
    "        while (target in subStr):\n",
    "            index = subStr.index(target)\n",
    "            subStr = subStr[index:]\n",
    "            tempURL = subStr.split()[0]\n",
    "            subStr = subStr[len(tempURL):]\n",
    "            tempURL = tempURL[len(target):]\n",
    "            if len(tempURL) <= 5:\n",
    "                continue\n",
    "            if \"http://\" in tempURL:\n",
    "                newURLList = tempURL.split(\"http://\")\n",
    "                if '' in newURLList:\n",
    "                    newURLList.remove('')\n",
    "                for newURLListItem in newURLList:\n",
    "                    if '/' in newURLListItem:\n",
    "                        newURLListItem = newURLListItem.split('/')[0]\n",
    "                        part = newURLListItem.split('.')\n",
    "                        urlList.append(\"{0}\".format(part[-2]))\n",
    "                continue\n",
    "            elif \"http:/\" in tempURL:\n",
    "                tempURL = tempURL.replace(\"http:/\", '')\n",
    "                if len(tempURL) < 5:\n",
    "                    continue\n",
    "            if '/' in tempURL:\n",
    "                tempURL = tempURL.split('/')[0]\n",
    "            part = tempURL.split('.')\n",
    "            while '' in part:\n",
    "                part.remove('')\n",
    "            if len(part) < 2:\n",
    "                continue\n",
    "            tempURL = \"{0}\".format(part[-2])\n",
    "            if '?' in tempURL:\n",
    "                part = tempURL.split(\"?\")\n",
    "                while '' in part:\n",
    "                    part.remove('')\n",
    "                tempURL = part[0]\n",
    "            urlList.append(tempURL)\n",
    "    return urlList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractEmail(sentence):\n",
    "    email_suffix = [\"@handle.com\", \"@handle.net\"]\n",
    "    for surfix in email_suffix:\n",
    "        if surfix in sentence:\n",
    "            email = sentence.split(surfix)[0]    \n",
    "            if \"\" != email and (\"@\" not in email):\n",
    "                return [email]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_words = [\"@handle\", \"http\", \"RT\"]\n",
    "urlRemoveList = [\"bit.ly\", \"ow.ly\", \"tinyurl\", \"short.to\", \"s1z.us\", \"s3z.us\", ]\n",
    "\n",
    "\n",
    "def pre_process(sentence, setence_len_limit=5):\n",
    "    sentence = sentence.split()\n",
    "    remain_tokens = []\n",
    "    target_remove = set()\n",
    "    for token in sentence:\n",
    "        lowerSubStr = token.lower()\n",
    "        urlRemoveFlag = False\n",
    "        for urlRemove in urlRemoveList:\n",
    "            if urlRemove in lowerSubStr:\n",
    "                urlRemoveFlag = True\n",
    "                break\n",
    "        if urlRemoveFlag:\n",
    "            continue\n",
    "        if (\"RT\" in token) and (len(token) < 4):\n",
    "            continue\n",
    "        if (\"http\" in token.lower()):\n",
    "            urlList = extractURL(token)\n",
    "            remain_tokens += urlList\n",
    "            continue\n",
    "        if (\"@handle\" in token):\n",
    "            email_list = extractEmail(token)\n",
    "            remain_tokens += email_list\n",
    "            continue\n",
    "        remain_tokens.append(token)\n",
    "            \n",
    "    sentence = remain_tokens\n",
    "    while \"\" in sentence:\n",
    "        sentence.remove(\"\")\n",
    "    while \"@\" in sentence:\n",
    "        sentence.remove(\"@\")\n",
    "    sentence_length = len(sentence)\n",
    "    if sentence_length < setence_len_limit:\n",
    "        return (\"\", 0)\n",
    "    else:\n",
    "        sentence = ' '.join(sentence)\n",
    "        return (sentence, sentence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 328931\n",
      "Total ids: 9297\n",
      "Total ids after filtering: 9287\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "random.seed(10)\n",
    "\n",
    "train_file_path = \"../data/train_tweets.txt\"\n",
    "train_dict = collections.defaultdict(list)\n",
    "\n",
    "length_array = []\n",
    "id_counter = set()\n",
    "with open(train_file_path, encoding='utf-8') as tsvfile:\n",
    "    reader = tsvfile.readlines()\n",
    "    for i, row in enumerate(reader):\n",
    "        row = row.strip().split(\"\\t\")\n",
    "        id = int(row[0])\n",
    "        id_counter.add(id)\n",
    "        (instance, sentence_length) = pre_process(row[1])\n",
    "        if not instance == \"\":\n",
    "            train_dict[id].append(instance)\n",
    "            length_array.append(sentence_length)\n",
    "    print(\"Total rows: %d\" % i)\n",
    "    \n",
    "print(\"Total ids: %d\" % len(id_counter))\n",
    "print(\"Total ids after filtering: %d\" % len(train_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitTrainDev(trainDict, tweets_num_limit=10):\n",
    "    dev_split = 0.1\n",
    "    train_split = 1 - dev_split\n",
    "\n",
    "    dev_set_dict = {}\n",
    "    train_set_dict = {}\n",
    "\n",
    "    for id in trainDict:\n",
    "        target_list = trainDict[id]\n",
    "        length = len(target_list)\n",
    "        if length < tweets_num_limit:\n",
    "            continue\n",
    "        random.shuffle(target_list)\n",
    "        split = int(np.ceil(length*dev_split))\n",
    "        dev_set_dict[id] = target_list[:split]\n",
    "        train_set_dict[id] = target_list[split:length]\n",
    "    #     print(len(dev_set_dict[id]), len(train_set_dict[id]), length)\n",
    "\n",
    "    print(len(dev_set_dict), len(train_set_dict), len(trainDict))\n",
    "    return (dev_set_dict, train_set_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(target_dict, file_path):\n",
    "    id_list = []\n",
    "    sentence_list = []\n",
    "    for id in target_dict:\n",
    "        for sentence in target_dict[id]:\n",
    "            id_list.append(id)\n",
    "            sentence_list.append(sentence)\n",
    "      \n",
    "    id_list = np.array(id_list)\n",
    "    sentence_list = np.array(sentence_list)\n",
    "    random_index = np.array(range(len(sentence_list)))\n",
    "    random.shuffle(random_index)\n",
    "    id_list = id_list[random_index]\n",
    "    sentence_list = sentence_list[random_index]\n",
    "    \n",
    "    dataframe = pd.DataFrame({'id':id_list,'sentence':sentence_list})\n",
    "    dataframe.to_csv(file_path,index=False,sep='\\t',header=None)\n",
    "    print(len(id_list))\n",
    "    return\n",
    "\n",
    "def save_idx(target_dict, file_path):\n",
    "    # Build IDX and Save\n",
    "    idx = {}\n",
    "    for i, id in enumerate(target_dict):\n",
    "        idx[id] = i\n",
    "    print(len(idx))\n",
    "    with open(file_path, 'wb') as handle:\n",
    "        pickle.dump(idx, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8791 8791 9287\n",
      "32827\n",
      "8791\n"
     ]
    }
   ],
   "source": [
    "version = \"v2_1\"\n",
    "dev_set_path = '../data/{0}/dev_set_{0}.txt'.format(version)\n",
    "train_set_path = '../data/{0}/train_set_{0}.txt'.format(version)\n",
    "idx_file_path = '../data/{0}/{0}_idx.pickle'.format(version)\n",
    "\n",
    "dev_set_dict, train_set_dict = splitTrainDev(train_dict)\n",
    "\n",
    "save_to_file(dev_set_dict, dev_set_path)\n",
    "with open(train_set_path, 'wb') as handle:\n",
    "    pickle.dump(train_set_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "save_idx(train_set_dict, idx_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
