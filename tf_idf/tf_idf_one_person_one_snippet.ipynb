{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.cluster.vq import whiten\n",
    "\n",
    "def get_tf_idf_query_similarity(docs_tfidf, query_tfidf):\n",
    "    cosineSimilarities = cosine_similarity(query_tfidf, docs_tfidf).flatten()\n",
    "    return cosineSimilarities\n",
    "\n",
    "# def l2_norm(a):\n",
    "#     return math.sqrt(np.dot(a, a))\n",
    "\n",
    "# def cosine_similarity(a, b):\n",
    "#     return np.dot(a,b) / (l2_norm(a) * l2_norm(b))\n",
    "\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "ascii_digits = '0123456789'\n",
    "ascii_lowercase = 'abcdefghijklmnopqrstuvwxyz'\n",
    "stops = stopwords.words('english')\n",
    "punctuation_str = string.punctuation\n",
    "punctuation_str_no_quote = punctuation_str.replace(\"\\'\", \"\")\n",
    "symbol_emoji_list = [\":)\",  \";)\",  \":(\",  \":\\\\\",  \":|\", \":]\", \":[\",\n",
    "                     \":-)\", \";-)\", \":-(\", \":-\\\\\", \":-|\", \":-[\", \";-]\",\n",
    "                     \":D\", \":P\", \":-x\", \":'-(\", \":_(\", \":o)\", \"XD\", \":'(\", \":->\",\n",
    "                     \"o_O\", \"T_T\", \"^o^\", \n",
    "                     \":-D\", \":-P\",\"B-)\", \"8-)\", \":-o\", \":-O\", \":-0\", \":-s\", \":-S\"]\n",
    "\n",
    "# TODO hashtag #\n",
    "# TODO hmmmmm hhuuuuugg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v2_1\"\n",
    "dev_set_path = '../data/{0}/dev_set_{0}.txt'.format(version)\n",
    "train_set_path = '../data/{0}/train_set_{0}.txt'.format(version)\n",
    "\n",
    "train_file = open(train_set_path, 'rb')\n",
    "train_data = pickle.load(train_file)\n",
    "dev_data = pd.read_csv(dev_set_path, header=None, sep='\\t')\n",
    "train_label = np.fromiter(train_data.keys(), dtype=int)\n",
    "dev_label = (np.array(dev_data)[:,0]).astype('int')\n",
    "#train_sentence_list = train_data[1]\n",
    "dev_sentence_list = dev_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainFeature(train_data):\n",
    "    \"\"\"\n",
    "    return:\n",
    "        (m, 287)\n",
    "        [avg_sentence_length(1), digit_percentage(1),       upper_case_percentage(1),   digit_frequency(10),\n",
    "         letter_frequency(26),   punctuation_frequency(32), symbol_emoji_frequency(35), function_words_frequency(179),\n",
    "         avg_word_len(1),        short_word_ratio(1)]\n",
    "    \"\"\"\n",
    "    train_len = len(train_data)\n",
    "    train_avg_sentence_length = np.zeros((train_len, 1))\n",
    "    train_sentence_num = np.zeros((train_len, 1))\n",
    "    \n",
    "    train_digit_percentage = np.zeros((train_len, 1))\n",
    "    train_upper_case_percentage = np.zeros((train_len, 1))\n",
    "    \n",
    "    train_digit_frequency = np.zeros((train_len, len(ascii_digits)))\n",
    "    train_letter_frequency = np.zeros((train_len, len(ascii_lowercase)))\n",
    "    \n",
    "    # punctuation_str = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    # symbol_emoji_list\n",
    "    train_punctuation_occurrence = np.zeros((train_len, len(punctuation_str)))\n",
    "    train_symbol_emoji_occurence = np.zeros((train_len, len(symbol_emoji_list)))\n",
    "    \n",
    "    train_function_words_frequency = np.zeros((train_len, len(stops)))\n",
    "    \n",
    "    train_avg_word_len = np.zeros((train_len, 1))\n",
    "    \n",
    "    train_short_word_ratio = np.zeros((train_len, 1))\n",
    "    \n",
    "    # train_vocab_richness = np.zeros((train_len, 2))\n",
    "    \n",
    "    train_content_words_dict = {}\n",
    "    \n",
    "    counter = 0\n",
    "    for key,values in train_data.items():\n",
    "        sentence_num = len(values)\n",
    "        whole_str = \" \".join(values)\n",
    "        \n",
    "        char_num = len(whole_str) - sentence_num + 1\n",
    "        train_sentence_num[counter] = sentence_num\n",
    "        train_avg_sentence_length[counter] = char_num/sentence_num\n",
    "        \n",
    "        upper_case_counter = 0\n",
    "        digit_counter = 0\n",
    "        for char in whole_str:\n",
    "            if char.isupper():\n",
    "                upper_case_counter += 1\n",
    "                continue\n",
    "            if char.isdigit():\n",
    "                digit_counter += 1\n",
    "                continue\n",
    "        train_digit_percentage[counter] = upper_case_counter/char_num\n",
    "        train_upper_case_percentage[counter] = digit_counter/char_num\n",
    "        \n",
    "        for index, char in enumerate(ascii_digits):\n",
    "            train_letter_frequency[counter, index] = whole_str.count(char) / char_num\n",
    "        \n",
    "        whole_str_lower = whole_str.lower()\n",
    "        for index, char in enumerate(ascii_lowercase):\n",
    "            train_letter_frequency[counter, index] = whole_str_lower.count(char) / char_num\n",
    "        \n",
    "        for index, punctuation in enumerate(punctuation_str):\n",
    "            train_punctuation_occurrence[counter, index] = whole_str.count(punctuation)\n",
    "        for index, symbol_emoji in enumerate(symbol_emoji_list):\n",
    "            train_symbol_emoji_occurence[counter, index] = whole_str.count(symbol_emoji)\n",
    "        \n",
    "        whole_str_lower_no_punctuation = whole_str_lower\n",
    "        for index, punctuation in enumerate(punctuation_str_no_quote):\n",
    "            if punctuation in whole_str_lower_no_punctuation:\n",
    "                whole_str_lower_no_punctuation = whole_str_lower_no_punctuation.replace(punctuation, \" \")\n",
    "        \n",
    "        word_list = whole_str_lower_no_punctuation.split()\n",
    "        word_num = len(word_list)\n",
    "        if 0 == word_num:\n",
    "            train_function_words_frequency[counter, index] = 0\n",
    "            train_avg_word_len[counter, 0] = 0\n",
    "            train_short_word_ratio[counter, 0] = 0\n",
    "            train_content_words_list.append([])\n",
    "            continue\n",
    "        \n",
    "        word_list_no_stop_word = whole_str_lower_no_punctuation.split()\n",
    "        for index, stop_word in enumerate(stops):\n",
    "            train_function_words_frequency[counter, index] = whole_str_lower_no_punctuation.count(stop_word) / word_num\n",
    "            while stop_word in word_list_no_stop_word:\n",
    "                word_list_no_stop_word.remove(stop_word)\n",
    "        \n",
    "        word_len = 0\n",
    "        short_word_num = 0\n",
    "        for word in word_list:\n",
    "            word_len_temp = len(word)\n",
    "            word_len += word_len_temp\n",
    "            if word_len_temp < 4:\n",
    "                short_word_num += 1\n",
    "        train_avg_word_len[counter, 0] = word_len / word_num\n",
    "        train_short_word_ratio[counter, 0] = short_word_num / word_num\n",
    "        \n",
    "\n",
    "        train_content_words_dict[key] = \" \".join(word_list_no_stop_word)\n",
    "\n",
    "    #     hapaxes = filter(lambda x: word_list.count(x) == 1, word_list)\n",
    "    #     hapaxes_ratio = len(hapaxes) / word_num\n",
    "    #     train_vocab_richness[counter, 0] = hapaxes_ratio\n",
    "    #     train_vocab_richness[counter, 1] = 1 - hapaxes_ratio\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "    train_punctuation_frequency = train_punctuation_occurrence / train_sentence_num\n",
    "    train_symbol_emoji_frequency = train_symbol_emoji_occurence / train_sentence_num\n",
    "    feature_mat = np.concatenate((train_avg_sentence_length, train_digit_percentage, train_upper_case_percentage,\n",
    "                                  train_digit_frequency, train_letter_frequency, train_punctuation_frequency,\n",
    "                                  train_symbol_emoji_frequency, train_function_words_frequency,\n",
    "                                  train_avg_word_len, train_short_word_ratio), axis=1)\n",
    "    assert(feature_mat.shape == (train_len, 287))\n",
    "    return (feature_mat, train_content_words_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDevFeature(dev_sentence_list):\n",
    "    \"\"\"\n",
    "    return:\n",
    "        (m, 287)\n",
    "        [avg_sentence_length(1), digit_percentage(1),       upper_case_percentage(1),   digit_frequency(10),\n",
    "         letter_frequency(26),   punctuation_frequency(32), symbol_emoji_frequency(35), function_words_frequency(179),\n",
    "         avg_word_len(1),        short_word_ratio(1)]\n",
    "    \"\"\"\n",
    "    dev_len = len(dev_sentence_list)\n",
    "    dev_avg_sentence_length = np.zeros((dev_len, 1))\n",
    "    \n",
    "    dev_digit_percentage = np.zeros((dev_len, 1))\n",
    "    dev_upper_case_percentage = np.zeros((dev_len, 1))\n",
    "    \n",
    "    dev_digit_frequency = np.zeros((dev_len, len(ascii_digits)))\n",
    "    dev_letter_frequency = np.zeros((dev_len, len(ascii_lowercase)))\n",
    "    \n",
    "    # punctuation_str = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    # symbol_emoji_list\n",
    "    dev_punctuation_frequency = np.zeros((dev_len, len(punctuation_str)))\n",
    "    dev_symbol_emoji_frequency = np.zeros((dev_len, len(symbol_emoji_list)))\n",
    "    \n",
    "    dev_function_words_frequency = np.zeros((dev_len, len(stops)))\n",
    "    \n",
    "    dev_avg_word_len = np.zeros((dev_len, 1))\n",
    "    \n",
    "    dev_short_word_ratio = np.zeros((dev_len, 1))\n",
    "    \n",
    "    dev_content_words_list = []\n",
    "    \n",
    "    counter = 0\n",
    "    for sentence in dev_sentence_list:\n",
    "        char_num = len(sentence)\n",
    "        dev_avg_sentence_length[counter] = char_num\n",
    "        \n",
    "        upper_case_counter = 0\n",
    "        digit_counter = 0\n",
    "        for char in sentence:\n",
    "            if char.isupper():\n",
    "                upper_case_counter += 1\n",
    "                continue\n",
    "            if char.isdigit():\n",
    "                digit_counter += 1\n",
    "                continue\n",
    "        dev_digit_percentage[counter] = upper_case_counter/char_num\n",
    "        dev_upper_case_percentage[counter] = digit_counter/char_num\n",
    "        \n",
    "        for index, char in enumerate(ascii_digits):\n",
    "            dev_letter_frequency[counter, index] = sentence.count(char) / char_num\n",
    "        \n",
    "        whole_str_lower = sentence.lower()\n",
    "        for index, char in enumerate(ascii_lowercase):\n",
    "            dev_letter_frequency[counter, index] = whole_str_lower.count(char) / char_num\n",
    "        \n",
    "        for index, punctuation in enumerate(punctuation_str):\n",
    "            dev_punctuation_frequency[counter, index] = sentence.count(punctuation)\n",
    "        for index, symbol_emoji in enumerate(symbol_emoji_list):\n",
    "            dev_symbol_emoji_frequency[counter, index] = sentence.count(symbol_emoji)\n",
    "        \n",
    "        whole_str_lower_no_punctuation = whole_str_lower\n",
    "        for index, punctuation in enumerate(punctuation_str_no_quote):\n",
    "            if punctuation in whole_str_lower_no_punctuation:\n",
    "                whole_str_lower_no_punctuation = whole_str_lower_no_punctuation.replace(punctuation, \" \")\n",
    "        \n",
    "        word_list = whole_str_lower_no_punctuation.split()\n",
    "        word_num = len(word_list)\n",
    "        if 0 == word_num:\n",
    "            dev_function_words_frequency[counter, index] = 0\n",
    "            dev_avg_word_len[counter, 0] = 0\n",
    "            dev_short_word_ratio[counter, 0] = 0\n",
    "            dev_content_words_list.append([])\n",
    "            continue\n",
    "\n",
    "        word_list_no_stop_word = whole_str_lower_no_punctuation.split()\n",
    "        for index, stop_word in enumerate(stops):\n",
    "            dev_function_words_frequency[counter, index] = whole_str_lower_no_punctuation.count(stop_word) / word_num\n",
    "            while stop_word in word_list_no_stop_word:\n",
    "                word_list_no_stop_word.remove(stop_word)\n",
    "        \n",
    "        word_len = 0\n",
    "        short_word_num = 0\n",
    "        for word in word_list:\n",
    "            word_len_temp = len(word)\n",
    "            word_len += word_len_temp\n",
    "            if word_len_temp < 4:\n",
    "                short_word_num += 1\n",
    "        dev_avg_word_len[counter, 0] = word_len / word_num\n",
    "        dev_short_word_ratio[counter, 0] = short_word_num / word_num\n",
    "        \n",
    "\n",
    "        dev_content_words_list.append(\" \".join(word_list_no_stop_word))\n",
    "        counter += 1\n",
    "    feature_mat = np.concatenate((dev_avg_sentence_length, dev_digit_percentage, dev_upper_case_percentage,\n",
    "                                  dev_digit_frequency, dev_letter_frequency, dev_punctuation_frequency,\n",
    "                                  dev_symbol_emoji_frequency, dev_function_words_frequency,\n",
    "                                  dev_avg_word_len, dev_short_word_ratio), axis=1)\n",
    "    assert(feature_mat.shape == (dev_len, 287))\n",
    "    return (feature_mat, dev_content_words_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21 s, sys: 63.4 ms, total: 21 s\n",
      "Wall time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_feature_mat, train_new_word_dict = getTrainFeature(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.6 s, sys: 57.6 ms, total: 4.66 s\n",
      "Wall time: 4.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dev_feature_mat, dev_new_word_list = getDevFeature(dev_sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 96.1 ms, sys: 4.52 ms, total: 101 ms\n",
      "Wall time: 99.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=-1, n_neighbors=13, p=2,\n",
       "                     weights='distance')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "k = 13\n",
    "knn_clf = KNeighborsClassifier(n_jobs=-1, n_neighbors=k, weights=\"distance\")\n",
    "knn_clf.fit(train_feature_mat, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.0012\n",
      "CPU times: user 24.3 s, sys: 1.95 s, total: 26.3 s\n",
      "Wall time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predicted = knn_clf.predict(dev_feature_mat)\n",
    "accuracy = accuracy_score(predicted, dev_label)\n",
    "print(\"accuracy: {0:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.3 s, sys: 585 ms, total: 39.9 s\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "k = 1000\n",
    "k_neighbors_list = knn_clf.kneighbors(dev_feature_mat, n_neighbors=k, return_distance=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_neighbors_label = train_label[k_neighbors_list.ravel()].reshape(k_neighbors_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12440978462850702\n"
     ]
    }
   ],
   "source": [
    "repeat_dev_label = np.tile(dev_label, (k,1)).T\n",
    "result_mat = np.sum(np.equal(k_neighbors_label, repeat_dev_label), axis=1)\n",
    "result_mat = (0 != result_mat).astype('int')\n",
    "print(np.mean(result_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../data/glove.6B.100d.txt\"\n",
    "import numpy as np\n",
    "def loadGloveModel(gloveFile):\n",
    "    print (\"Loading Glove Model\")\n",
    "    \n",
    "     \n",
    "    with open(gloveFile, encoding=\"utf8\" ) as f:\n",
    "        content = f.readlines()\n",
    "    model = {}\n",
    "    for line in content:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print (\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "     \n",
    "     \n",
    "model= loadGloveModel(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.spatial import distance\n",
    "def cosine_distance_countvectorizer_method(s1, s2):\n",
    "    # sentences to list\n",
    "    allsentences = [s1 , s2]\n",
    "    \n",
    "    # text to vector\n",
    "    vectorizer = CountVectorizer()\n",
    "    all_sentences_to_vector = vectorizer.fit_transform(allsentences)\n",
    "    text_to_vector_v1 = all_sentences_to_vector.toarray()[0].tolist()\n",
    "    text_to_vector_v2 = all_sentences_to_vector.toarray()[1].tolist()\n",
    "    \n",
    "    # distance of similarity\n",
    "    cosine = distance.cosine(text_to_vector_v1, text_to_vector_v2)\n",
    "    #print('Similarity of two sentences are equal to ',round((1-cosine)*100,2),'%')\n",
    "    return cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "\n",
    "def preprocess(raw_text):\n",
    "\n",
    "    # keep only words\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "\n",
    "    # convert to lower case and split \n",
    "    words = letters_only_text.lower().split()\n",
    "\n",
    "    # remove stopwords\n",
    "    stopword_set = set(stopwords.words(\"english\"))\n",
    "    cleaned_words = list(set([w for w in words if w not in stopword_set]))\n",
    "\n",
    "    return cleaned_words\n",
    "\n",
    "def cosine_distance_between_two_words(word1, word2):\n",
    "    import scipy\n",
    "    return (1- scipy.spatial.distance.cosine(model[word1], model[word2]))\n",
    "\n",
    "def calculate_heat_matrix_for_two_sentences(s1,s2):\n",
    "    s1 = preprocess(s1)\n",
    "    s2 = preprocess(s2)\n",
    "    result_list = [[cosine_distance_between_two_words(word1, word2) for word2 in s2] for word1 in s1]\n",
    "    result_df = pd.DataFrame(result_list)\n",
    "    result_df.columns = s2\n",
    "    result_df.index = s1\n",
    "    return result_df\n",
    "\n",
    "def cosine_distance_wordembedding_method(s1, s2):\n",
    "    vector_1 = np.mean([model[word] for word in preprocess(s1)],axis=0)\n",
    "    vector_2 = np.mean([model[word] for word in preprocess(s2)],axis=0)\n",
    "    cosine = scipy.spatial.distance.cosine(vector_1, vector_2)\n",
    "    print('Word Embedding method with a cosine distance asses that our two sentences are similar to',round((1-cosine)*100,2),'%')\n",
    "\n",
    "def wordembedding_method(s1):\n",
    "    return np.mean([model[word] for word in s1],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_csv = \"../data/v1_4/train_set_v1_4.txt\"\n",
    "dev_set_csv = \"../data/v1_4/dev_set_v1_4.txt\"\n",
    "\n",
    "train_csv = pd.read_csv(train_set_csv, sep='\\t', header=None)\n",
    "train_csv = np.array(train_csv)\n",
    "dev_csv = pd.read_csv(dev_set_csv, sep='\\t', header=None)\n",
    "dev_csv = np.array(dev_csv)\n",
    "\n",
    "train_label = (train_csv[:,0]).astype('int')\n",
    "train_sentence_list = train_csv[:,1]\n",
    "dev_label = (dev_csv[:,0]).astype('int')\n",
    "dev_sentence_list = dev_csv[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cleaned_train_sentence_list = []\n",
    "for train_sentence in train_sentence_list:\n",
    "    cleaned_train_sentence_list.append(preprocess(train_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cleaned_dev_sentence_list = []\n",
    "for dev_sentence in dev_sentence_list:\n",
    "    cleaned_dev_sentence_list.append(preprocess(dev_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train_sentence_df = pd.DataFrame(cleaned_train_sentence_list)\n",
    "cleaned_dev_sentence_df = pd.DataFrame(cleaned_dev_sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train_sentence_vec = cleaned_train_sentence_df.apply(wordembedding_method)\n",
    "cleaned_dev_sentence_vec = cleaned_dev_sentence_df.apply(wordembedding_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for dev_sentence in dev_sentence_list:\n",
    "    for train_sentence in train_sentence_list:\n",
    "        cosine_similarity = cosine_distance_countvectorizer_method(dev_sentence, train_sentence)\n",
    "    #print(cosine_similarity)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
