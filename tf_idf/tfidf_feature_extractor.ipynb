{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.cluster.vq import whiten\n",
    "\n",
    "def get_tf_idf_query_similarity(docs_tfidf, query_tfidf):\n",
    "    cosineSimilarities = cosine_similarity(query_tfidf, docs_tfidf).flatten()\n",
    "    return cosineSimilarities\n",
    "\n",
    "# def l2_norm(a):\n",
    "#     return math.sqrt(np.dot(a, a))\n",
    "\n",
    "# def cosine_similarity(a, b):\n",
    "#     return np.dot(a,b) / (l2_norm(a) * l2_norm(b))\n",
    "\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "stops = stopwords.words('english')\n",
    "punctuation_str = string.punctuation\n",
    "symbol_emoji_list = [\":)\",  \";)\",  \":(\",  \":\\\\\",  \":|\", \":]\", \":[\",\n",
    "                     \":-)\", \";-)\", \":-(\", \":-\\\\\", \":-|\", \":-[\", \";-]\",\n",
    "                     \":D\", \":P\", \":-x\", \":'-(\", \":_(\", \":o)\", \"XD\", \":'(\", \":->\",\n",
    "                     \"o_O\", \"T_T\", \"^o^\", \n",
    "                     \":-D\", \":-P\",\"B-)\", \"8-)\", \":-o\", \":-O\", \":-0\", \":-s\", \":-S\"]\n",
    "# TODO hashtag #\n",
    "# TODO hmmmmm hhuuuuugg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v1_4\"\n",
    "version_name = \"{0}\".format(version)\n",
    "dev_set_path = 'data/{0}/dev_set_{1}.txt'.format(version, version_name)\n",
    "train_set_path = 'data/{0}/train_set_{1}.txt'.format(version, version_name)\n",
    "\n",
    "train_data = pd.read_csv(train_set_path, sep='\\t', header=None)\n",
    "dev_data = pd.read_csv(dev_set_path, sep='\\t', header=None)\n",
    "train_label = (np.array(train_data)[:,0]).astype('int')\n",
    "dev_label = (np.array(dev_data)[:,0]).astype('int')\n",
    "train_sentence_list = train_data[1]\n",
    "dev_sentence_list = dev_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.4 s, sys: 88.2 ms, total: 34.5 s\n",
      "Wall time: 34.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from nltk import word_tokenize\n",
    "\n",
    "train_sentence_tokenized = train_sentence_list.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 275 ms, sys: 12 ms, total: 287 ms\n",
      "Wall time: 286 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_list = []\n",
    "for index, value in train_sentence_tokenized.iteritems():\n",
    "    all_list += value\n",
    "corpus = set(all_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dict = dict(zip(corpus, range(len(corpus))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 4 µs, total: 4 µs\n",
      "Wall time: 268 µs\n"
     ]
    }
   ],
   "source": [
    "# 建立句子的向量表示\n",
    "def vector_rep(text, corpus_dict):\n",
    "    vec = []\n",
    "    for key in corpus_dict.keys():\n",
    "        if key in text:\n",
    "            vec.append((corpus_dict[key], text.count(key)))\n",
    "        else:\n",
    "            vec.append((corpus_dict[key], 0))\n",
    "\n",
    "    vec = sorted(vec, key= lambda x: x[0])\n",
    "    return vec\n",
    "\n",
    "from math import sqrt\n",
    "def similarity_with_2_sents(vec1, vec2):\n",
    "    inner_product = 0\n",
    "    square_length_vec1 = 0\n",
    "    square_length_vec2 = 0\n",
    "    for tup1, tup2 in zip(vec1, vec2):\n",
    "        inner_product += tup1[1]*tup2[1]\n",
    "        square_length_vec1 += tup1[1]**2\n",
    "        square_length_vec2 += tup2[1]**2\n",
    "\n",
    "    return (inner_product/sqrt(square_length_vec1*square_length_vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mvector_rep\u001b[0;34m(text, corpus_dict)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for index, value in dev_sentence_list.iteritems():\n",
    "    dev_vec = vector_rep(value, corpus_dict)\n",
    "    sim_array = np.zeros((len(dev_sentence_list),1))\n",
    "    for _, train_value in train_sentence_list.iteritems():\n",
    "        train_vec = vector_rep(train_value, corpus_dict)\n",
    "        sim_array[index, 0] = similarity_with_2_sents(dev_vec, train_vec)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(PorterStemmer().stem(item))\n",
    "    return stems\n",
    "\n",
    "token_dict = {}\n",
    "for sentence in train_sentence_list:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(word_tokens):\n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        if w not in stops: \n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get part of speech for each token in each chapter\n",
    "def token_to_pos(ch):\n",
    "    tokens = nltk.word_tokenize(ch)\n",
    "    return [p[1] for p in nltk.pos_tag(tokens)]\n",
    "\n",
    "def featureExtractor(sentence_data):\n",
    "    NUM_TOP_WORDS = 10\n",
    "    pos_list = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS']\n",
    "    \n",
    "    num_users = len(sentence_data)\n",
    "    # fvs stands for feature vectors\n",
    "    fvs_lexical = np.zeros((num_users, 3), np.float64)\n",
    "    fvs_punct = np.zeros((num_users, len(punctuation_str)), np.float64)\n",
    "    fvs_symbol_emoji = np.zeros((num_users, len(symbol_emoji_list)), np.float64)\n",
    "    fvs_bow = []\n",
    "#     fvs_syntax = []\n",
    "    \n",
    "    for i, sentense_txt in enumerate(sentence_data):\n",
    "        # note: the nltk.word_tokenize includes punctuation\n",
    "        all_text = \" \".join(sentense_txt)\n",
    "        tokens = nltk.word_tokenize(all_text.lower())\n",
    "        words = word_tokenizer.tokenize(all_text.lower())\n",
    "        vocab = set(words)\n",
    "    \n",
    "        words_per_sentence = np.array([len(word_tokenizer.tokenize(s))\n",
    "                                       for s in sentense_txt])\n",
    "        # average number of words per sentence\n",
    "        fvs_lexical[i, 0] = words_per_sentence.mean()\n",
    "        # sentence length variation\n",
    "        fvs_lexical[i, 1] = words_per_sentence.std()\n",
    "        # Lexical diversity\n",
    "        fvs_lexical[i, 2] = len(vocab) / (float(len(words))+1)\n",
    " \n",
    "        for j, char in enumerate(punctuation_str):\n",
    "            fvs_punct[i, j] = tokens.count(char) / (float(len(sentense_txt))+1)\n",
    "        \n",
    "        for j, emoji in enumerate(symbol_emoji_list):\n",
    "            fvs_symbol_emoji[i, j] = tokens.count(emoji) / (float(len(sentense_txt))+1)\n",
    "\n",
    "    fvs_lexical = np.nan_to_num(fvs_lexical)\n",
    "    fvs_punct = np.nan_to_num(fvs_punct)\n",
    "    fvs_lexical = whiten(fvs_lexical)\n",
    "    fvs_punct = whiten(fvs_punct)\n",
    "    return (fvs_lexical, fvs_punct, fvs_symbol_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTFIDF(sentence_data):\n",
    "    #sentense_list = sentence_data\n",
    "    tfidf_list = []\n",
    "    mat_list = []\n",
    "    for sentense_txt in sentence_data:\n",
    "        # note: the nltk.word_tokenize includes punctuation\n",
    "        # all_text = \" \".join(sentense_txt)\n",
    "        #sentense_txt = list(sentense_txt)\n",
    "        if 0 == len(sentense_txt):\n",
    "            tfidf_list.append(None)\n",
    "            mat_list.append(None)\n",
    "            break\n",
    "        vectorizer = TfidfVectorizer(lowercase=False, stop_words=None)\n",
    "        X = vectorizer.fit(sentense_txt)\n",
    "        mat = vectorizer.fit_transform(sentense_txt)\n",
    "        tfidf_list.append(X)\n",
    "        mat_list.append(mat)\n",
    "    return (tfidf_list, mat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_list, mat_list = getTFIDF(train_data['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePrediction(sentence):\n",
    "    prediction = np.zeros((len(train_label),1))\n",
    "    for i, tfidf in enumerate(tfidf_list):\n",
    "        X, mat = tfidf, mat_list[i]\n",
    "        if (X == None) or (mat == None):\n",
    "            prediction[i] = -1\n",
    "            break\n",
    "        temp_mat = X.transform(sentence)\n",
    "        sim = get_tf_idf_query_similarity(mat, temp_mat)\n",
    "        prediction[i] = max(sim)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.zeros((len(dev_label),1))\n",
    "for i, sentence in enumerate(dev_data['sentence']):\n",
    "    sim = makePrediction(sentence)\n",
    "    prediction[i] = train_label[np.argmax(max(sim))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.15 s, sys: 41.4 ms, total: 6.19 s\n",
      "Wall time: 6.27 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/sml/lib/python3.7/site-packages/scipy/cluster/vq.py:139: RuntimeWarning: Some columns have standard deviation zero. The values of these columns will not change.\n",
      "  RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dev_feature = featureExtractor(dev_data['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/sml/lib/python3.7/site-packages/ipykernel_launcher.py:28: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/envs/sml/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/anaconda3/envs/sml/lib/python3.7/site-packages/numpy/core/_methods.py:140: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  keepdims=keepdims)\n",
      "/anaconda3/envs/sml/lib/python3.7/site-packages/numpy/core/_methods.py:110: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/anaconda3/envs/sml/lib/python3.7/site-packages/numpy/core/_methods.py:132: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.3 s, sys: 72 ms, total: 36.4 s\n",
      "Wall time: 36.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_feature = featureExtractor(train_data['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=-1, n_neighbors=13, p=2,\n",
       "                     weights='distance')"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO PCA, TF-IDF\n",
    "k=13\n",
    "\n",
    "train_lexical, train_punct, train_symbol_emoji = train_feature\n",
    "dev_lexical, dev_punct, dev_symbol_emoji = dev_feature\n",
    "\n",
    "train_fit_data = np.concatenate((train_lexical, train_punct, train_symbol_emoji), axis=1)\n",
    "dev_fit_data = np.concatenate((dev_lexical, dev_punct, dev_symbol_emoji), axis=1)\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_jobs=-1, n_neighbors=k, weights=\"distance\")\n",
    "knn_clf.fit(train_fit_data, train_label)\n",
    "\n",
    "# lexical_clf = KNeighborsClassifier(n_jobs=-1, n_neighbors=k, weights=\"distance\")\n",
    "# punct_clf = KNeighborsClassifier(n_jobs=-1, n_neighbors=k, weights=\"distance\")\n",
    "# lexical_clf.fit(train_lexical, train_label)\n",
    "# punct_clf.fit(train_punct, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010112963959117805\n",
      "CPU times: user 12.7 s, sys: 52.4 ms, total: 12.7 s\n",
      "Wall time: 4.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predicted = knn_clf.predict(dev_fit_data)\n",
    "accuracy = sum(predicted == dev_label)/len(predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00021516944593867672\n",
      "CPU times: user 1.21 s, sys: 23 ms, total: 1.23 s\n",
      "Wall time: 1.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#lexical_knn = train_label[lexical_clf.kneighbors(dev_lexical, return_distance=False)]\n",
    "lexical_predicted = lexical_clf.predict(dev_lexical)\n",
    "accuracy = sum(lexical_predicted == dev_label)/len(lexical_predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n",
      "CPU times: user 8.71 s, sys: 50.2 ms, total: 8.76 s\n",
      "Wall time: 4.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#punct_knn = train_label[punct_clf.kneighbors(dev_punct, return_distance=False)]\n",
    "punct_predicted = punct_clf.predict(dev_punct)\n",
    "#accuracy = sum(punct_predicted == dev_label)/len(punct_predicted)\n",
    "accuracy = sum(punct_predicted == dev_label)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = np.concatenate((lexical_knn, punct_knn),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "predicted_list = np.zeros((len(dev_label),1))\n",
    "counter = 0\n",
    "for i, candidate in enumerate(predicted):\n",
    "    if dev_label[i] in candidate:\n",
    "        counter += 1\n",
    "    predicted_list[i] = np.argmax(np.bincount(candidate))\n",
    "accuracy = sum(predicted_list.astype('int') == np.array(dev_label)) / len(dev_label)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9295"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fvs_bow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
