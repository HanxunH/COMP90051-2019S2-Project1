{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# from extract_v15 import FeatureExtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    m = len(X)\n",
    "    mini_batches = []\n",
    "\n",
    "    num_complete_minibatches = int(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = X[k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "        mini_batch_Y = Y[k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = X[num_complete_minibatches * mini_batch_size:]\n",
    "        mini_batch_Y = Y[num_complete_minibatches * mini_batch_size:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "def readData(data_path, mini_batch_size=1):\n",
    "    df_data = pd.read_csv(data_path, sep='\\t', header=None)\n",
    "    df_data = np.array(df_data)\n",
    "\n",
    "    label_list = (df_data[:,0]).astype('int')\n",
    "    sentence_list = df_data[:,1]\n",
    "    if 1 == mini_batch_size:\n",
    "        return sentence_list, label_list\n",
    "    else:\n",
    "        mini_batches = random_mini_batches(sentence_list, label_list, mini_batch_size)\n",
    "        return mini_batches\n",
    "    \n",
    "def getFeatureList(sentence_list, use_batch=False):\n",
    "    if use_batch:\n",
    "        first_sencence_list = sentence_list[0][0]\n",
    "        feature_list = np.asarray(test_model.encode(first_sencence_list, batch_size=32))\n",
    "        for batch in sentence_list[1:]:\n",
    "            feature = np.asarray(test_model.encode(list(batch[0]), batch_size=32))\n",
    "            feature_list = np.concatenate((feature_list, feature))\n",
    "    else:\n",
    "        feature_list = []\n",
    "        for sentence in sentence_list:\n",
    "            feature = test_model.get_features([sentence])\n",
    "            feature_list.append(np.ravel(feature))\n",
    "        feature_list = np.array(feature_list)\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_version = \"v13_copy\"\n",
    "# model_path = \"checkpoints/{0}.pth\".format(model_version)\n",
    "\n",
    "train_data_path = \"data/v7/train_set_v1_7.txt\"\n",
    "dev_data_path = \"data/v7/dev_set_v1_7.txt\"\n",
    "\n",
    "sentence_list, label_list = readData(train_data_path)\n",
    "dev_sentence, dev_label = readData(dev_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Training and Dev set feature list\n",
    "# test_model = FeatureExtract(checkpoints_path=model_path)\n",
    "from sentence_transformers import SentenceTransformer \n",
    "test_model = SentenceTransformer('checkpoints/sentence_transformers/bert_base_cased_200_v1_epoch7')\n",
    "\n",
    "mini_batches  = readData(train_data_path, 128)\n",
    "feature_list = getFeatureList(mini_batches, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dev_mini_batches  = readData(dev_data_path, 32)\n",
    "dev_feature_list = getFeatureList(dev_mini_batches, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_path = \"data/v5train_siamese_bert_base_cased_200_v1_epoch7\"\n",
    "dev_feature_path = \"data/v5dev_siamese_bert_base_cased_200_v1_epoch7\"\n",
    "\n",
    "np.save(train_feature_path, feature_list)\n",
    "np.save(dev_feature_path, dev_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_path = 'data/v7/train_siamese_bert_base_cased_200_v7.1_triplet_epoch1.npy'\n",
    "dev_feature_path = 'data/v7/dev_siamese_bert_base_cased_200_v7.1_triplet_epoch1.npy'\n",
    "feature_list = np.load(train_feature_path)\n",
    "dev_feature_list = np.load(dev_feature_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36184, 768) 292748\n",
      "(292748, 768) 292748\n"
     ]
    }
   ],
   "source": [
    "print(dev_feature_list.shape, len(label_list))\n",
    "print(feature_list.shape, len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2330311739995578\n",
      "CPU times: user 17min 47s, sys: 1min 11s, total: 18min 58s\n",
      "Wall time: 6min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# metric='cosine'\n",
    "k = 13\n",
    "knn_clf = KNeighborsClassifier(n_jobs=-1, metric='cosine', n_neighbors=k, weights='distance')\n",
    "knn_clf.fit(feature_list, label_list)\n",
    "predicted = knn_clf.predict(dev_feature_list)\n",
    "accuracy = sum(predicted == dev_label)/len(predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "k=20\n",
    "k_neighbors_list = knn_clf.kneighbors(X=dev_feature_list, n_neighbors=k, return_distance=False)\n",
    "# k_neigbbors_path = \"data/lemon_bear_files/k_neighbors_list_k_eq_{0}\".format(k)\n",
    "# np.save(k_neigbbors_path, k_neighbors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_neigbbors_npy_path = \"data/lemon_bear_files/k_neighbors_list_k_eq_{0}.npy\".format(k)\n",
    "# k_neighbors_list = np.load(k_neigbbors_npy_path)\n",
    "k_neighbors_index_list = k_neighbors_list\n",
    "label_k_neighbors_np = []\n",
    "for i, index_list in enumerate(k_neighbors_index_list):\n",
    "    line = []\n",
    "    line.append(dev_label[i])\n",
    "    candidate = list(label_list[index_list.astype('int')])\n",
    "    line.append(candidate)\n",
    "    label_k_neighbors_np.append(line)\n",
    "label_k_neighbors_np = np.array(label_k_neighbors_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall:0.3939586557594517\n"
     ]
    }
   ],
   "source": [
    "true_label_list = label_k_neighbors_np[:,0]\n",
    "candidate_label_list = label_k_neighbors_np[:,1]\n",
    "counter = 0\n",
    "for i, label in enumerate(true_label_list):\n",
    "    if label in candidate_label_list[i]:\n",
    "        counter += 1\n",
    "candidate_accuracy = counter / len(true_label_list)\n",
    "print(\"recall:{0}\".format(candidate_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.648269953570638\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i, label in enumerate(true_label_list):\n",
    "    counter += len(np.unique(candidate_label_list[i]))\n",
    "avg_user_counter = counter / len(candidate_label_list)\n",
    "print(avg_user_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
