{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from extract_v6 import FeatureExtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    m = len(X)\n",
    "    mini_batches = []\n",
    "\n",
    "    num_complete_minibatches = int(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = X[k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "        mini_batch_Y = Y[k * mini_batch_size : (k + 1) * mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = X[num_complete_minibatches * mini_batch_size:]\n",
    "        mini_batch_Y = Y[num_complete_minibatches * mini_batch_size:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "def readData(data_path, mini_batch_size=1):\n",
    "    df_data = pd.read_csv(data_path, sep='\\t', header=None)\n",
    "    df_data = np.array(df_data)\n",
    "\n",
    "    label_list = (df_data[:,0]).astype('int')\n",
    "    sentence_list = df_data[:,1]\n",
    "\n",
    "    if 1 == mini_batch_size:\n",
    "        return sentence_list, label_list\n",
    "    else:\n",
    "        mini_batches = random_mini_batches(sentence_list, label_list, mini_batch_size)\n",
    "        return mini_batches\n",
    "    \n",
    "def getFeatureList(sentence_list, use_batch=False):\n",
    "    if use_batch:\n",
    "        first_sencence_list = sentence_list[0][0]\n",
    "        feature_list = test_model.get_features(first_sencence_list)\n",
    "        for batch in sentence_list[1:]:\n",
    "            feature = test_model.get_features(list(batch[0]))\n",
    "            feature_list = np.concatenate((feature_list, feature))\n",
    "    else:\n",
    "        feature_list = []\n",
    "        for sentence in sentence_list:\n",
    "            feature = test_model.get_features([sentence])\n",
    "            feature_list.append(np.ravel(feature))\n",
    "        feature_list = np.array(feature_list)\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = \"margin5.0_v6_epoch2\"\n",
    "model_path = \"checkpoints/{0}.pth\".format(model_version)\n",
    "train_data_path = \"lemonBearData/train_set_v1.txt\"\n",
    "dev_data_path = \"lemonBearData/dev_set_v1.txt\"\n",
    "\n",
    "sentence_list, label_list = readData(train_data_path)\n",
    "dev_sentence, dev_label = readData(dev_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract Model V6 FeatureOnly\n",
      "CPU times: user 42min 35s, sys: 5min 30s, total: 48min 6s\n",
      "Wall time: 10min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training and Dev set feature list\n",
    "test_model = FeatureExtract(checkpoints_path=model_path)\n",
    "mini_batches  = readData(train_data_path, 64)\n",
    "feature_list = getFeatureList(mini_batches, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 36s, sys: 21.2 s, total: 4min 58s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dev_mini_batches  = readData(dev_data_path, 64)\n",
    "dev_feature_list = getFeatureList(dev_mini_batches, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_path = \"lemonBearData/train_{0}\".format(model_version)\n",
    "dev_feature_path = \"lemonBearData/dev_{0}\".format(model_version)\n",
    "\n",
    "np.save(train_feature_path, feature_list)\n",
    "np.save(dev_feature_path, dev_feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09065994906433396\n",
      "CPU times: user 43min 31s, sys: 4.38 s, total: 43min 36s\n",
      "Wall time: 8min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "k = 13\n",
    "knn_clf = KNeighborsClassifier(n_jobs=-1, n_neighbors=k, weights=\"distance\")\n",
    "knn_clf.fit(feature_list, label_list)\n",
    "predicted = knn_clf.predict(dev_feature_list)\n",
    "accuracy = sum(predicted == dev_label)/len(predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44min 19s, sys: 4.09 s, total: 44min 23s\n",
      "Wall time: 8min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "k=20\n",
    "k_neighbors_list = knn_clf.kneighbors(X=dev_feature_list, n_neighbors=k, return_distance=False)\n",
    "k_neigbbors_path = \"lemonBearData/k_neighbors_list_k_eq_{0}\".format(k)\n",
    "np.save(k_neigbbors_path, k_neighbors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_neigbbors_npy_path = \"lemonBearData/k_neighbors_list_k_eq_{0}.npy\".format(k)\n",
    "k_neighbors_list = np.load(k_neigbbors_npy_path)\n",
    "k_neighbors_index_list = k_neighbors_list\n",
    "label_k_neighbors_np = []\n",
    "for i, index_list in enumerate(k_neighbors_index_list):\n",
    "    line = []\n",
    "    line.append(dev_label[i])\n",
    "    candidate = list(label_list[index_list.astype('int')])\n",
    "    line.append(candidate)\n",
    "    label_k_neighbors_np.append(line)\n",
    "label_k_neighbors_np = np.array(label_k_neighbors_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall:0.15665485549773003\n"
     ]
    }
   ],
   "source": [
    "true_label_list = label_k_neighbors_np[:,0]\n",
    "candidate_label_list = label_k_neighbors_np[:,1]\n",
    "counter = 0\n",
    "for i, label in enumerate(true_label_list):\n",
    "    if label in candidate_label_list[i]:\n",
    "        counter += 1\n",
    "candidate_accuracy = counter / len(true_label_list)\n",
    "print(\"recall:{0}\".format(candidate_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.53161333185694\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i, label in enumerate(true_label_list):\n",
    "    counter += len(np.unique(candidate_label_list[i]))\n",
    "avg_user_counter = counter / len(candidate_label_list)\n",
    "print(avg_user_counter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
