{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre Processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_words = [\"@handle\", \"RT\", \"http\"]\n",
    "\n",
    "def pre_process(sentence, max_length):\n",
    "    sentence = sentence.split()\n",
    "    target_remove = set()\n",
    "    for token in sentence:\n",
    "        for target in remove_words:\n",
    "            if target in token:\n",
    "                target_remove.add(token)\n",
    "                \n",
    "    for target in target_remove:\n",
    "        sentence.remove(target)\n",
    "    max_length = max(max_length, len(sentence))\n",
    "    sentence = ' '.join(sentence)\n",
    "    return sentence, max_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Train data to train_dict. \n",
    "train_dict[id] = [[train_instace1], [train_instance2] ....]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 328194\n",
      "Total ids: 9292\n",
      "Longest Sentence: 1061\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import collections\n",
    "\n",
    "train_file_path = \"data/train_tweets.txt\"\n",
    "train_dict = collections.defaultdict(list)\n",
    "max_length = 0\n",
    "\n",
    "with open(train_file_path, encoding='utf-8') as tsvfile:\n",
    "    reader = csv.reader((x.replace('\\0', '') for x in tsvfile), delimiter='\\t')\n",
    "    for i, row in enumerate(reader):\n",
    "        id = int(row[0])\n",
    "        instance, max_length = pre_process(row[1], max_length)\n",
    "        if not instance == \"\":\n",
    "            train_dict[id].append(instance)\n",
    "    print(\"Total rows: %d\" % i)\n",
    "    \n",
    "print(\"Total ids: %d\" % len(train_dict))\n",
    "print(\"Longest Sentence: %d\" % (max_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I'm looking for a recommendation for a leasing call center other than level one. Any suggestions?\", \"I'm at triple rock brewing in Berkeley...great spot to watch the Yankees win!\", 'whostalkin.com is a great source as well #AptChat', 'I also use google alerts on all our comps #aptchat', 'Hi! Matt here from RECP. I also use google alerts....I usually receive craigslist ads or apartment ratings from the alerts #aptchat', 'Shopping apartments on Oakland today....so far I like what I see', 'RT Real-time social media stats from @handle', 'Does your social class determine your online social network? - #cnn', 'The new Paramount Theatre sign is up! They are going to light it on 10/21/09 at 6:00 pm!! You should all Come!', 'CL now asking for phone numbers to verify your posting.Looks like it is localized to SoCal so far. Anyone else seeing this?', 'I humbly remind U that bad reviews R not the problem, but a symptom', \"UDR's augmented reality apartment search application passes 25,000 downloads\", 'Off to dempression...any tubers in the house?', 'it worth buying a tv to see it?', 'Social Media Facts - Did You Know 4.0', \"oday's Blue Angels show cancelled due to heavy fog:\", 'timing is everything...happy to be a part instead of reading transcripts', 'apartment communities have the advantage over every other business because we already have a physical comm. to leverage SM tools #REAchat', 'SM tools are great for spreading information through your peer network. Pictures/videos illustrate what you are selling #REAchat', 'Looking forward to #REAchat with and @handle and @handle', \"The Poke at Yabbi's is amazing.....props to the waiter for hooking us up\", \"Here's one list of twitter chat's any others?\", \"AptChat Today at 4 pm ET - More Attention on Retention! w/ & @handle from Satisfacts. Don't miss it! #AptChat\", \"Hey Apartment Peeps...New blog post - Transparency - Don't leave your residents in the dark -\", \"Camden's Ric Campo credits at least 37 leases directly from Twitter. Rentwiki integrates w/Twitter #NMHC via\", 'Petshop boys at the warfield', 'Alta Phoenix Lofts #1 Phoenix!!!!! Congrats to all involved', 'great party last night...met some talented people', 'A Small Business Guide to Text-Message Marketing -', 'Please add me to the #awsms09 afterparty. thanks', 'Great bike ride to GG Bridge ...now headed to watch Il Trovatore at AT&T park.', 'Community events are the key ingredient to resident retention', 'you should get a fan page so all your fans can cheer you on', 'Tomorrow: \"Is Print Advertising Really as Dead as Some People Say?\" Bring a friend & your best ideas - 4 pm ET. #AptChat', 'New Blog Posting', \"Check out our newest post 'Building Community Relationships through Activities'\", 'You need an Iphone Optimized Website', 'Social Media', 'Great tips for creating facebook fan', 'Free Web Seminar for Property Managers: 5 Steps to Maximize Your Online Marketing Efforts!', 'to Bad Online Customer Reviews #AptChat', 'An apartment veteran shares some lab experiments in social media marketing | ViralHousingFix', 'Stay on top of your social media game', 'from burning man to Denver to SF...time to wash off the playa dust', 'Community events are the key ingredient to resident retention', 'Great bike ride to GG Bridge ...now headed to watch Il Trovatore at AT&T park.', 'Launched the new socially connected Settlers Creek', 'Please add me to the #awsms09 afterparty. thanks', 'great party last night...met some talented people', 'Alta Phoenix Lofts #1 Phoenix!!!!! Congrats to all involved']\n"
     ]
    }
   ],
   "source": [
    "print(train_dict[1319])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Dev Train\n",
    "!! There are Some ID have 0 train instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9292 9292 9292\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "dev_split = 0.1\n",
    "train_split = 1 - dev_split\n",
    "\n",
    "dev_set_dict = {}\n",
    "train_set_dict = {}\n",
    "\n",
    "for id in train_dict:\n",
    "    target_list = train_dict[id]\n",
    "    length = len(target_list)\n",
    "    random.shuffle(target_list)\n",
    "    split = int(np.ceil(length*dev_split))\n",
    "    dev_set_dict[id] = target_list[:split]\n",
    "    train_set_dict[id] = target_list[split:length]\n",
    "#     print(len(dev_set_dict[id]), len(train_set_dict[id]), length)\n",
    "\n",
    "print(len(dev_set_dict), len(train_set_dict), len(train_dict))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build ID idx and train/dev set save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36056\n",
      "291190\n",
      "9292\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def save_to_file(target_dict, file_path):\n",
    "    file_lines = []\n",
    "    for id in target_dict:\n",
    "        for sentence in target_dict[id]:\n",
    "            file_lines.append(str(id) + '\\t' + sentence + '\\n')\n",
    "            \n",
    "    random.shuffle(file_lines)\n",
    "    print(len(file_lines))\n",
    "#     with open(file_path, 'w+') as file:\n",
    "#         for item in file_lines:\n",
    "#             file.write(item)\n",
    "    return\n",
    "\n",
    "dev_set_path = 'data/v1/dev_set_v1.txt'\n",
    "train_set_path = 'data/v1/train_set_v1.txt'\n",
    "idx_file_path = 'data/v1/v1_idx.pickle'\n",
    "\n",
    "save_to_file(dev_set_dict, dev_set_path)\n",
    "save_to_file(train_set_dict, train_set_path)\n",
    "\n",
    "\n",
    "# Build IDX and Save\n",
    "idx = {}\n",
    "for i, id in enumerate(train_dict):\n",
    "    idx[id] = i\n",
    "print(len(idx))\n",
    "\n",
    "with open(idx_file_path, 'wb') as handle:\n",
    "    pickle.dump(idx, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
